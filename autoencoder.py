# -*- coding: utf-8 -*-
"""Quest-4-AUTOENCODER .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gQpZc0TMUefuITxt2UC5_hLXOqI0eIn
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torchvision import datasets, transforms

import numpy as np
import matplotlib.pyplot as plt
import copy

plt.style.use('default')

transform = transforms.ToTensor()

traindataset = datasets.MNIST('.', download=True, train=True, transform=transform)
testdataset = datasets.MNIST('.', download=True, train=False, transform=transform)

bs = 512
trainloader = torch.utils.data.DataLoader(traindataset, batch_size=bs, shuffle=True, num_workers=4)
testloader = torch.utils.data.DataLoader(testdataset, batch_size=bs, shuffle=False, num_workers=4)

print(trainloader.dataset)

class AutoEncoder(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(in_features=kwargs["input_shape"], out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=64),
            nn.ReLU())

        self.decoder = nn.Sequential(
            nn.Linear(in_features=64, out_features=128),
            nn.ReLU(),
            nn.Linear(in_features=128, out_features=256),
            nn.ReLU(),
            nn.Linear(in_features=256, out_features=kwargs["input_shape"]),
            nn.ReLU())
        

    def forward(self, features):
        enc = self.encoder(features)
        reconstructed =  self.decoder(enc)
        return reconstructed

#  use gpu if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# create a model from `AE` autoencoder class
# load it to the specified device, either gpu or cpu
model = AutoEncoder(input_shape=784).to(device)

# create an optimizer object
# Adam optimizer with learning rate 1e-3
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# mean-squared error loss
criterion = nn.MSELoss()

net = AutoEncoder(input_shape=784)
net

if torch.cuda.is_available():
  GPU = 1
  print('Training on GPU')
  net = net.cuda()
# init_weights = copy.deepcopy(net.encoder.weight.data)

# init_weights = copy.deepcopy(net.encoder.weight.data)

optimizer = optim.SGD(net.parameters(), lr=0.5, momentum=0.9)
criterion_1 = nn.MSELoss()
criterion_2 = nn.CrossEntropyLoss()

epochs = 25
AutoencoderLoss = []
for epoch in range(epochs):
  runningloss = 0
  total_correct = 0
  for images, labels in trainloader:
    if GPU:
       
      images, labels = Variable(images.view(images.size()[0], -1)).cuda(), Variable(labels).cuda()
    
    optimizer.zero_grad()
    output = net(images)
    loss = criterion_1(output, images.view(images.size()[0], -1))
    # loss_2 = criterion_2(classifier_out, labels)
    # loss = loss_1+ loss_2
    loss.backward()
    optimizer.step()
    runningloss += loss.item()/images.shape[0]
  #   _, pred = torch.max(classifier_out, 1)
  #   total_correct += torch.sum(pred==labels).item()
  # accuracy = total_correct / len(trainloader.dataset) *100
  AutoencoderLoss.append(runningloss)
  print('Epoch: {}/{} \t Mean Square Error Loss: {}'.format(epoch+1, epochs, runningloss))
import matplotlib.pyplot as plt
plt.plot(AutoencoderLoss)
plt.xlabel('epoch')
plt.ylabel('AutoencoderLoss loss')
plt.show()

test_images, labels = next(iter(testloader))

if GPU:
  test_images = Variable(images.view(images.size()[0], -1)).cuda()
  test_output = net(test_images)
  
inp = test_images.view(-1, 28, 28)
out = test_output.view(-1, 28, 28)

fig = plt.figure()
plot = fig.add_subplot(1, 2, 1)
plot.set_title('Original Image')
imgplot = plt.imshow(inp[0].cpu(), cmap='gray')

plot = fig.add_subplot(1, 2, 2)
plot.set_title('Generated Image')
imgplot = plt.imshow(out[0].cpu().detach(), cmap='gray')
plt.show()

new_classif = nn.Sequential(*list(net.children())[:-1])
net = new_classif
net.add_module('classifier', nn.Sequential(nn.Linear(64, 10), nn.LogSoftmax(dim=1)))
print(net)

if GPU:
  net = net.cuda()
  
cll_weights = copy.deepcopy(net[0][0].weight.data)
init_classifier_weights = copy.deepcopy(net.classifier[0].weight.data)

criterion_C = nn.NLLLoss()
optimizer_C = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)

epochs = 20
classifierLoss = []
for epoch in range(epochs):
  runningloss = 0
  total_correct = 0
  
  for images, labels in trainloader:
    if GPU:
      images, labels = Variable(images.view(images.size()[0], -1)).cuda(), Variable(labels).cuda()
#     print(images.shape, labels.shape)
    output = net(images)
    optimizer_C.zero_grad()
    loss = criterion_C(output, labels)
    loss.backward()
    optimizer_C.step()
    runningloss += loss.item()/images.shape[0]
    _, pred = torch.max(output, 1)
    total_correct += torch.sum(pred==labels).item()

  accuracy = total_correct / len(trainloader.dataset)
  classifierLoss.append(runningloss)
  print('Epoch: {}/{} \t Training Loss: {}, Train Accuracy: {}'.format(epoch+1, epochs, runningloss, accuracy))
  
  if(True):
    net.eval()
    total_correct = 0
    with torch.no_grad():
      for images, labels in testloader:
        if GPU:
          images, labels = Variable(images.view(images.size()[0], -1)).cuda(), Variable(labels).cuda()
          output_acc = net(images)
          _, pred = torch.max(output_acc, 1)
          total_correct += torch.sum(pred==labels).item()
    accuracy = total_correct / len(testloader.dataset)
  print('Epoch: {}/{} \t Training Loss: {}, Test Accuracy: {}'.format(epoch+1, epochs, runningloss, accuracy))

import matplotlib.pyplot as plt
plt.plot(classifierLoss)
plt.xlabel('epoch')
plt.ylabel('training classifier loss')
plt.show()

net